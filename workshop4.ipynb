{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting data - this may take a moment\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "############### LOCAL IMPORTS ###############\n",
    "from network_classes import MLP \n",
    "from stats_and_plot_functions import mean_squared_error, calc_rhoc, plot_loss_and_r, CustomScaler\n",
    "\n",
    "\n",
    "torch.manual_seed(42) # 42 for reproducibility\n",
    "num=100 # Use this for testing and debugging\n",
    "#num=None set to this when you want it to load\n",
    "cwd = os.getcwd()\n",
    "data_train = pd.read_csv(cwd+'/data/train_data_10k_conjugated_cores.csv')[:num]\n",
    "data_test = pd.read_csv(cwd+'/data/test_data_500_analogues.csv') \n",
    "\n",
    "print(\"Formatting data - this may take a moment\")\n",
    "# Training data\n",
    "train_Etddft = data_train['S1_TDDFT'].values \n",
    "train_Ezindo = data_train['S1_ZINDO'].values\n",
    "train_fp = np.array([np.array(ast.literal_eval(fp), dtype=np.int32) for fp in data_train['fingerprint'].values])\n",
    "net_input= np.concatenate((train_Ezindo.reshape(-1,1), train_fp),axis=1)\n",
    "\n",
    "# Testing data (NOT VALIDATION DATA, use this once you model has been fully trained)\n",
    "test_Etddft = data_test['E(S1_TDDFT)_analogue'].values \n",
    "test_Ezindo = data_test['E(S1_ZINDO)_analogue'].values\n",
    "test_fp = np.array([np.array(ast.literal_eval(fp), dtype=np.int32) for fp in data_test['fingerprint'].values])\n",
    "\n",
    "\n",
    "# Initialise model\n",
    "criterion = nn.MSELoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jan 29 19:44:39 2025\n",
    "\n",
    "Author: Adam Coxson, PhD student, University of Liverpool\n",
    "Department of Chemistry, Materials Innovation Factory, Levershulme Research Centre\n",
    "Module: network_models.py\n",
    "Local dependencies: network_classes.py, stats_and_plot_functions.py\n",
    "For the dML workshop Mar 2025\n",
    "\n",
    "# https://medium.com/@shashankshankar10/introduction-to-neural-networks-build-a-single-layer-perceptron-in-pytorch-c22d9b412ccf\n",
    "# Numpy only perceptron: https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html\n",
    "# https://www.kaggle.com/code/pinocookie/pytorch-simple-mlp\n",
    "\n",
    "Trains a neural network on 1D regression data and visualises the results.\n",
    "\"\"\"\n",
    "\n",
    "############## PACKAGE IMPORTS ###############\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "############### LOCAL IMPORTS ###############\n",
    "from network_classes import SingleLayerPerceptron, MLP_2layer, MLP \n",
    "from stats_and_plot_functions import compute_y, sample_x_values, normalize_data, mean_squared_error, plot_train_loss, doubleplot, calc_num_net_parameters\n",
    "\n",
    "\n",
    "torch.manual_seed(42) # 42 for reproducibility\n",
    "\n",
    "# Uncomment to show the True function of ((0.4*x + 0.5*np.sin(5*x) + np.sin(3*x)) + 10*np.cos(x)*np.exp(-0.1*x)) + 7\n",
    "x_min, x_max, dx = 0, 30, 0.001\n",
    "num_samples = 50\n",
    "x_values = np.linspace(x_min, x_max, 300)\n",
    "y_values = compute_y(x_values)\n",
    "# Randomly sample x values and compute corresponding y values\n",
    "sampled_x = sample_x_values(x_min, x_max, dx, num_samples)\n",
    "sampled_y = compute_y(sampled_x)\n",
    "# doubleplot(x_values, y_values, sampled_x, sampled_y,labels=[\"x\",\"y\",\"True function\",\"50 Random 'training' points\",\"\"],lims=[[0,30],[0,20]])\n",
    "# exit()\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 1 # We only have one input dimension (the scalar x value)\n",
    "output_dim = 1  # We only have one output dimension (the scalar y value)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "neurons=[100,50,20,10]\n",
    "#neurons=5000\n",
    "#neurons=[100, 50]\n",
    "# See list of different activation functions https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
    "activation = [nn.SELU(), nn.ReLU(), nn.Sigmoid()][1] \n",
    "dropout=0.0  # Careful with this as it may currently be broken\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "#model = SingleLayerPerceptron(input_dim=1, hidden_dim=neurons, output_dim=1,activation_func=activation)\n",
    "#model = MLP_2layer(input_dim=1,output_dim=1,neurons=neurons,activation_func=activation)\n",
    "layers=[] # Format layers for variable MLP class\n",
    "for n in neurons:\n",
    "    layers.append((n, activation))\n",
    "model = MLP(input_dim,output_dim,layers,dropout_prob=dropout)\n",
    "\n",
    "if type(neurons) is list:\n",
    "    nparams= calc_num_net_parameters(neurons.copy(), output_size=1)\n",
    "else:\n",
    "    nparams= calc_num_net_parameters(neurons, output_size=1)\n",
    "\n",
    "# Training data params\n",
    "training_samples=20000 \n",
    "x_min, x_max=0,30\n",
    "\n",
    "# Create and preprocess training data into 32 bit floats for PyTorch torch.Tensor objects\n",
    "x_vals_train=sample_x_values(x_min, x_max, dx=0.00001, num_samples=training_samples).astype(np.float32)\n",
    "y_vals_train = compute_y(x_vals_train).astype(np.float32)\n",
    "x_vals_train_normalized, x_mean, x_std = normalize_data(x_vals_train)\n",
    "y_vals_train_normalized, y_mean, y_std = normalize_data(y_vals_train)\n",
    "X_train = torch.tensor(x_vals_train_normalized.reshape(len(x_vals_train),1), device='cpu')\n",
    "y_train = torch.tensor(y_vals_train_normalized.reshape(len(y_vals_train),1), device='cpu')\n",
    "\n",
    "# Create Validation data. In this simple case we are just sampling the true function, but realisitically it is taken from\n",
    "# shuffled training data, i.e. for k-fold cross validation.\n",
    "x_vals_valid=np.arange(x_min, x_max + 0.01, 0.1).astype(np.float32)\n",
    "y_vals_valid = compute_y(x_vals_valid).astype(np.float32)\n",
    "x_vals_valid_normalised = (x_vals_valid - x_mean) / x_std # DELIBERATELY Using normalisation from TRAINING data\n",
    "y_vals_valid_normalised = (y_vals_valid - y_mean) / y_std # DELIBERATELY Using normalisation from TRAINING data\n",
    "X_valid = torch.tensor(x_vals_valid_normalised.reshape(len(x_vals_valid),1), device='cpu')\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with validation tracking\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_valid_loss, best_train_loss, best_epoch = float(\"inf\"), float(\"inf\"), 0\n",
    "best_model_state = None  # To store the best model state dict\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train() # Tell model we are in training mode, weights can be modified, later we switch to evaluation mode\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_X, batch_y = batch_X.float(), batch_y.float()\n",
    "        outputs = model(batch_X).squeeze()  # Ensure output shape matches target\n",
    "        loss = criterion(outputs, batch_y.squeeze())  # Compute loss\n",
    "        loss.backward()  # Apply backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval() # Important to tell the model we're in evaluation model, ensures no weights are changed by accident\n",
    "    with torch.no_grad():\n",
    "        valid_pred_normalized = np.squeeze(model(X_valid).detach().numpy()) # Predict validation data\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    valid_loss = mean_squared_error(y_vals_valid_normalised, valid_pred_normalized)  # Loss of true norm vs pred norm validation data\n",
    "    train_losses.append(train_loss), valid_losses.append(valid_loss)\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_epoch = epoch\n",
    "        best_valid_loss, best_train_loss = valid_loss, train_loss\n",
    "        best_model_state = model.state_dict().copy()  # Save model state, not the whole model object\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.5f}, Valid Loss: {valid_loss:.5f}\")\n",
    "print(\"\\nTraining complete.\\n\")\n",
    "print(\"Number of training samples:\",training_samples)\n",
    "print(\"Neurons:\",neurons,\"(\"+str(nparams)+\" parameters)\",\"\\nNum Epochs:\", num_epochs,\n",
    "      \"\\nLearning Rate:\",learning_rate,\"\\nBatch Size:\",batch_size,\"\\nActivation:\",activation)\n",
    "# Restore the best model before evaluation\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"Best model from Epoch {best_epoch+1} restored with Training Loss: {best_train_loss:.5f} and Validation Loss: {best_valid_loss:.5f}\")     \n",
    "plot_train_loss(train_losses, valid_losses, labels=[\"Epoch\", \"MSE Loss\", \"Training and Validation Loss\"])\n",
    "\n",
    "# Final evaluation using the best model\n",
    "model.eval() # Important to tell the model we're in evaluation model, ensures no weights are changed by accident\n",
    "with torch.no_grad():\n",
    "    valid_pred_normalized = np.squeeze(model(X_valid).detach().numpy())\n",
    "y_vals_valid_predicted = (valid_pred_normalized * y_std) + y_mean # Unnormalise \n",
    "\n",
    "\"\"\"\n",
    "####################### VISUALISE AND COMPARE THE TRUE AND PREDICTED DATA #######################\n",
    "\n",
    "How good was the network at predicting the true function? Try different hyperparamters and samplings of the data.\n",
    "\"\"\"\n",
    "doubleplot(x_vals_valid, y_vals_valid, x_vals_valid, y_vals_valid_predicted, labels=[\"x\",\"y\",\"True validation data\",\"Pred validation data\",\"\"])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Modified: Complete training pipeline with MLP class compatibility\n",
    "\"\"\"\n",
    "\n",
    "############## PACKAGE IMPORTS ###############\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "############### LOCAL IMPORTS ###############\n",
    "from network_classes import MLP \n",
    "from stats_and_plot_functions import mean_squared_error, calc_rhoc, CustomScaler\n",
    "\n",
    "# 固定随机种子保证可复现性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 超参数配置\n",
    "HIDDEN_LAYERS = [\n",
    "    (512, nn.ReLU()),  # 增加神经元数量\n",
    "    (256, nn.ReLU()),\n",
    "    (128, nn.ReLU())    # 增加隐藏层数\n",
    "]\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 500\n",
    "VALID_RATIO = 0.2\n",
    "\n",
    "# 加载数据\n",
    "cwd = os.getcwd()\n",
    "data_train = pd.read_csv(cwd+'/data/train_data_10k_conjugated_cores.csv')\n",
    "data_test = pd.read_csv(cwd+'/data/test_data_500_analogues.csv') \n",
    "\n",
    "# 数据预处理函数\n",
    "def prepare_data(input_data, target_data, scaler=None, fit=False):\n",
    "    \"\"\"数据标准化处理\"\"\"\n",
    "    if fit:\n",
    "        scaler = CustomScaler()\n",
    "        scaler.fit(input_data)\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    return torch.FloatTensor(input_scaled), torch.FloatTensor(target_data), scaler\n",
    "\n",
    "# 提取训练数据\n",
    "train_Etddft = data_train['S1_TDDFT'].values \n",
    "train_Ezindo = data_train['S1_ZINDO'].values\n",
    "train_fp = np.array([np.array(ast.literal_eval(fp), dtype=np.int32) for fp in data_train['fingerprint'].values])\n",
    "train_input = np.concatenate((train_Ezindo.reshape(-1,1), train_fp), axis=1)\n",
    "\n",
    "# 划分训练集/验证集\n",
    "train_input_split, val_input_split, train_target_split, val_target_split = train_test_split(\n",
    "    train_input, train_Etddft, test_size=VALID_RATIO, random_state=42\n",
    ")\n",
    "\n",
    "# 标准化（仅用训练集拟合）\n",
    "train_input_norm, train_target_norm, scaler = prepare_data(train_input_split, train_target_split, fit=True)\n",
    "val_input_norm, val_target_norm, _ = prepare_data(val_input_split, val_target_split, scaler=scaler)\n",
    "\n",
    "# 转换为PyTorch DataLoader\n",
    "train_dataset = torch.utils.data.TensorDataset(train_input_norm, train_target_norm)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 初始化模型（参数严格匹配MLP类定义）\n",
    "model = MLP(\n",
    "    input_dim    = train_input.shape[1],  # 输入特征维度 = ZINDO能量(1) + 指纹长度\n",
    "    output_dim   = 1,                     # 输出TDDFT能量\n",
    "    layers_data  = HIDDEN_LAYERS,         # 隐藏层配置\n",
    "    dropout_prob = 0.0                    # 无dropout\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 训练循环\n",
    "train_losses, val_losses = [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(-1,1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(val_input_norm)\n",
    "        val_loss = criterion(val_pred, val_target_norm.view(-1,1)).item()\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:03d}/{EPOCHS} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "# 测试集处理\n",
    "test_Etddft = data_test['E(S1_TDDFT)_analogue'].values \n",
    "test_Ezindo = data_test['E(S1_ZINDO)_analogue'].values\n",
    "test_fp = np.array([np.array(ast.literal_eval(fp), dtype=np.int32) for fp in data_test['fingerprint'].values])\n",
    "test_input = np.concatenate((test_Ezindo.reshape(-1,1), test_fp), axis=1)\n",
    "test_input_norm, _, _ = prepare_data(test_input, test_Etddft, scaler=scaler)\n",
    "test_input_tensor = torch.FloatTensor(test_input_norm)\n",
    "\n",
    "# 预测与评估\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = model(test_input_tensor).numpy().flatten()\n",
    "test_pred_actual = scaler.inverse_target(test_pred.reshape(-1,1)).flatten() if hasattr(scaler, 'inverse_target') else test_pred\n",
    "\n",
    "# 计算指标\n",
    "test_mse = mean_squared_error(test_Etddft, test_pred_actual)\n",
    "test_rho = calc_rhoc(test_Etddft, test_pred_actual)\n",
    "\n",
    "# 可视化结果\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.title(\"Training/Validation Loss Curve\")\n",
    "plt.xlabel(\"Epoch\"), plt.ylabel(\"MSE\"), plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(test_Etddft, test_pred_actual, alpha=0.5)\n",
    "plt.plot([min(test_Etddft), max(test_Etddft)], [min(test_Etddft), max(test_Etddft)], 'r--')\n",
    "plt.title(f\"TDDFT True vs Pred (ρ={test_rho:.3f})\")\n",
    "plt.xlabel(\"True Values\"), plt.ylabel(\"Predictions\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"delta_ml_results.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Test MSE: {test_mse:.5f}\")\n",
    "print(f\"Test Correlation: {test_rho:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda3)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
